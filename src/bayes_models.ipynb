{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3ea547f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 00:12:46.312547: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from lib.dataset import split_training_validation\n",
    "import pandas as pd\n",
    "from lib.trees import get_tree,parse_edge_list\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import networkx as nx\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from xgboost import XGBClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941c5d1d",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8669c41b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "language",
         "rawType": "category",
         "type": "unknown"
        },
        {
         "name": "sentence",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "n",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "edgelist",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "root",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "tree",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "ref": "57181de4-6c8b-4519-9052-9ef3e157f1bb",
       "rows": [
        [
         "0",
         "Japanese",
         "2",
         "23",
         "[(6, 4), (2, 6), (2, 23), (20, 2), (15, 20), (3, 5), (14, 3), (8, 14), (12, 8), (15, 12), (9, 15), (18, 9), (13, 18), (19, 13), (21, 19), (19, 1), (1, 22), (10, 21), (16, 7), (17, 16), (10, 17), (10, 11)]",
         "10",
         "Graph with 23 nodes and 22 edges"
        ],
        [
         "1",
         "Japanese",
         "5",
         "18",
         "[(8, 9), (14, 8), (4, 14), (5, 4), (1, 2), (6, 1), (17, 12), (6, 17), (6, 3), (7, 6), (11, 7), (7, 16), (16, 18), (15, 11), (5, 15), (10, 5), (10, 13)]",
         "10",
         "Graph with 18 nodes and 17 edges"
        ],
        [
         "2",
         "Japanese",
         "8",
         "33",
         "[(2, 10), (2, 14), (4, 2), (16, 4), (6, 16), (12, 32), (26, 12), (3, 26), (29, 27), (22, 29), (15, 22), (24, 18), (13, 24), (15, 13), (6, 15), (11, 5), (11, 23), (11, 21), (19, 11), (19, 20), (19, 33), (19, 8), (9, 19), (7, 9), (1, 28), (25, 1), (30, 25), (7, 30), (7, 17), (31, 7), (6, 31), (3, 6)]",
         "3",
         "Graph with 33 nodes and 32 edges"
        ],
        [
         "3",
         "Japanese",
         "11",
         "30",
         "[(30, 1), (14, 24), (21, 14), (3, 21), (7, 3), (12, 7), (7, 27), (27, 16), (8, 5), (26, 8), (6, 26), (12, 6), (25, 12), (22, 25), (25, 18), (18, 15), (15, 28), (22, 29), (19, 22), (23, 19), (13, 23), (10, 13), (11, 2), (30, 11), (17, 20), (9, 17), (4, 9), (10, 4), (30, 10)]",
         "30",
         "Graph with 30 nodes and 29 edges"
        ],
        [
         "4",
         "Japanese",
         "12",
         "19",
         "[(19, 13), (16, 19), (2, 16), (4, 10), (4, 15), (4, 5), (14, 4), (12, 14), (3, 12), (2, 3), (1, 8), (1, 18), (1, 7), (17, 1), (6, 17), (2, 6), (11, 2), (11, 9)]",
         "11",
         "Graph with 19 nodes and 18 edges"
        ]
       ],
       "shape": {
        "columns": 6,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>sentence</th>\n",
       "      <th>n</th>\n",
       "      <th>edgelist</th>\n",
       "      <th>root</th>\n",
       "      <th>tree</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Japanese</td>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>[(6, 4), (2, 6), (2, 23), (20, 2), (15, 20), (...</td>\n",
       "      <td>10</td>\n",
       "      <td>(6, 4, 2, 23, 20, 15, 3, 5, 14, 8, 12, 9, 18, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Japanese</td>\n",
       "      <td>5</td>\n",
       "      <td>18</td>\n",
       "      <td>[(8, 9), (14, 8), (4, 14), (5, 4), (1, 2), (6,...</td>\n",
       "      <td>10</td>\n",
       "      <td>(8, 9, 14, 4, 5, 1, 2, 6, 17, 12, 3, 7, 11, 16...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Japanese</td>\n",
       "      <td>8</td>\n",
       "      <td>33</td>\n",
       "      <td>[(2, 10), (2, 14), (4, 2), (16, 4), (6, 16), (...</td>\n",
       "      <td>3</td>\n",
       "      <td>(2, 10, 14, 4, 16, 6, 12, 32, 26, 3, 29, 27, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Japanese</td>\n",
       "      <td>11</td>\n",
       "      <td>30</td>\n",
       "      <td>[(30, 1), (14, 24), (21, 14), (3, 21), (7, 3),...</td>\n",
       "      <td>30</td>\n",
       "      <td>(30, 1, 14, 24, 21, 3, 7, 12, 27, 16, 8, 5, 26...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Japanese</td>\n",
       "      <td>12</td>\n",
       "      <td>19</td>\n",
       "      <td>[(19, 13), (16, 19), (2, 16), (4, 10), (4, 15)...</td>\n",
       "      <td>11</td>\n",
       "      <td>(19, 13, 16, 2, 4, 10, 15, 5, 14, 12, 3, 1, 8,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   language  sentence   n                                           edgelist  \\\n",
       "0  Japanese         2  23  [(6, 4), (2, 6), (2, 23), (20, 2), (15, 20), (...   \n",
       "1  Japanese         5  18  [(8, 9), (14, 8), (4, 14), (5, 4), (1, 2), (6,...   \n",
       "2  Japanese         8  33  [(2, 10), (2, 14), (4, 2), (16, 4), (6, 16), (...   \n",
       "3  Japanese        11  30  [(30, 1), (14, 24), (21, 14), (3, 21), (7, 3),...   \n",
       "4  Japanese        12  19  [(19, 13), (16, 19), (2, 16), (4, 10), (4, 15)...   \n",
       "\n",
       "   root                                               tree  \n",
       "0    10  (6, 4, 2, 23, 20, 15, 3, 5, 14, 8, 12, 9, 18, ...  \n",
       "1    10  (8, 9, 14, 4, 5, 1, 2, 6, 17, 12, 3, 7, 11, 16...  \n",
       "2     3  (2, 10, 14, 4, 16, 6, 12, 32, 26, 3, 29, 27, 2...  \n",
       "3    30  (30, 1, 14, 24, 21, 3, 7, 12, 27, 16, 8, 5, 26...  \n",
       "4    11  (19, 13, 16, 2, 4, 10, 15, 5, 14, 12, 3, 1, 8,...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = pd.read_csv(\"../data/train.csv\")\n",
    "sentences[\"language\"] = sentences[\"language\"].astype(\"category\")\n",
    "sentences[\"edgelist\"] = sentences[\"edgelist\"].apply(parse_edge_list)\n",
    "sentences[\"tree\"] = sentences[\"edgelist\"].apply(get_tree)\n",
    "sentences.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6050003",
   "metadata": {},
   "source": [
    "# Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5e74c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 8400\n",
      "Validation set size: 2100\n"
     ]
    }
   ],
   "source": [
    "random.seed(121)\n",
    "training, validation = split_training_validation(sentences, 0.2)\n",
    "\n",
    "print(\"Training set size:\", len(training))\n",
    "print(\"Validation set size:\", len(validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a7028ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "\n",
    "def get_longest_paths(graph) -> List[List]:\n",
    "    \"\"\"\n",
    "    Find all longest simple path in an undirected graph.\n",
    "    \"\"\"\n",
    "    longest_paths = []\n",
    "    longest_path_length = 0\n",
    "    for source in graph.nodes():\n",
    "        for target in graph.nodes():\n",
    "            if source == target:\n",
    "                continue\n",
    "            # Find all simple paths between source and target\n",
    "            paths = nx.all_simple_paths(graph, source=source, target=target)\n",
    "            for path in paths:\n",
    "                if len(path) > longest_path_length:\n",
    "                    longest_path_length = len(path)\n",
    "                    longest_paths.clear()\n",
    "                    longest_paths.append(path)\n",
    "                elif len(path) == longest_path_length:\n",
    "                    longest_paths.append(path)\n",
    "    return longest_paths\n",
    "\n",
    "\n",
    "def unwind_tree(row: pd.Series) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Unwind a tree into a list of nodes, with their centrality scores.\n",
    "    \"\"\"\n",
    "    tree: nx.Graph = row[\"tree\"]\n",
    "    root_node = row[\"root\"]\n",
    "    language = row[\"language\"]\n",
    "\n",
    "    # Centrality measures\n",
    "    eccentricity = nx.eccentricity(tree)  # Maximum distance to any other node in the tree.\n",
    "    closeness_centrality = nx.closeness_centrality(tree)  # Reciprocal avg distance to all other nodes in the tree.\n",
    "    degree_centrality = nx.degree_centrality(tree)  # Fraction of nodes that a node is connected to.\n",
    "    betweenness_centrality = nx.betweenness_centrality(tree)  # Fraction of shortest paths that pass through.\n",
    "    harmonic_centrality = nx.harmonic_centrality(tree)  # Average distance to all other nodes in the tree.\n",
    "    pagerank = nx.pagerank(tree)  # PageRank algorithm, which measures the importance of nodes.\n",
    "    katz_centrality = nx.katz_centrality(tree)  # It's a generalization of eigenvector centrality.\n",
    "    current_flow_closeness = nx.current_flow_closeness_centrality(tree)  # Closeness cent. based on resistance.\n",
    "    current_flow_betweenness = nx.current_flow_betweenness_centrality(tree)  # Betweenness cent. based on resistance.\n",
    "    load_centrality = nx.load_centrality(tree)  # Similar to betweenness centrality.\n",
    "    percolation_centrality = nx.percolation_centrality(tree)  # Proportion of \"percolation\" paths through a node.\n",
    "    second_order_centrality = nx.second_order_centrality(tree)  # Std of return times in perpetual random walk.\n",
    "    laplacian_centrality = nx.laplacian_centrality(tree)  # Centrality based on the Laplacian matrix.\n",
    "\n",
    "    # Tree properties\n",
    "    tree_diameter = nx.diameter(tree)  # Length of the longest path in the tree.\n",
    "    # Get the centroids of the tree\n",
    "    centroids = nx.center(tree)\n",
    "    # Get the leaves of the tree\n",
    "    leaves = [node for node, degree in tree.degree() if degree == 1]\n",
    "    # Get the longest path in the tree\n",
    "    longest_path = get_longest_paths(tree)\n",
    "    longest_path_nodes = set()\n",
    "    for path in longest_path:\n",
    "        longest_path_nodes.update(path)\n",
    "\n",
    "    rows = []\n",
    "    for node in tree:\n",
    "        # Create a tree rooted at the current node\n",
    "        tree_rooted_at_node: nx.DiGraph = nx.bfs_tree(tree, source=node)\n",
    "        # We can do this because we know it will be a tree, so no cycles exist\n",
    "        # and thus, bfs_edges will give us a valid tree structure.\n",
    "\n",
    "        subtrees: list[nx.DiGraph] = []\n",
    "        for child in tree.neighbors(node):\n",
    "            subtrees.append(nx.bfs_tree(tree, source=child))\n",
    "\n",
    "        subtree_leaves = []\n",
    "        for subtree in subtrees:\n",
    "            subtree_leaves.append([n for n in subtree if subtree.out_degree[n] == 0])\n",
    "\n",
    "        subtree_depths = []\n",
    "        for subtree in subtrees:\n",
    "            subtree_depths.append(nx.dag_longest_path_length(subtree))\n",
    "\n",
    "        rows.append(\n",
    "            {\n",
    "                \"row_index\": row.name,\n",
    "                \"node\": node,\n",
    "                \"is_root\": node == root_node,\n",
    "                # Global properties\n",
    "                \"language\": language,\n",
    "                \"tree_diameter\": tree_diameter,\n",
    "                \"tree_size\": tree.number_of_nodes(),\n",
    "                \"tree_edges\": tree.number_of_edges(),\n",
    "                \"number_of_centroids\": len(centroids),\n",
    "                \"average_degree\": sum(dict(tree.degree()).values()) / tree.number_of_nodes(),\n",
    "                \"number_of_leaves\": len(leaves),\n",
    "                # Local Node Properties\n",
    "                \"degree\": tree.degree[node],\n",
    "                \"is_leaf\": tree.degree[node] == 1,  # Only a leaf can have degree 1\n",
    "                \"is_centroid\": node in centroids,\n",
    "                # Path properties\n",
    "                \"distance_to_closest_centroid\": min(\n",
    "                    nx.shortest_path_length(tree, source=c, target=node) for c in centroids\n",
    "                ),\n",
    "                \"distance_to_farthest_centroid\": max(\n",
    "                    nx.shortest_path_length(tree, source=c, target=node) for c in centroids\n",
    "                ),\n",
    "                \"distance_to_closest_leaf\": min(nx.shortest_path_length(tree, source=l, target=node) for l in leaves),\n",
    "                \"distance_to_farthest_leaf\": max(nx.shortest_path_length(tree, source=l, target=node) for l in leaves),\n",
    "                \"is_in_longest_path\": node in longest_path_nodes,\n",
    "                # Properties of tree rooted at this node\n",
    "                \"tree_depth_if_root\": nx.dag_longest_path_length(tree_rooted_at_node),\n",
    "                \"min_subtree_size_if_root\": min(subtree.number_of_nodes() for subtree in subtrees) if subtrees else 0,\n",
    "                \"max_subtree_size_if_root\": max(subtree.number_of_nodes() for subtree in subtrees) if subtrees else 0,\n",
    "                \"average_subtree_size_if_root\": (\n",
    "                    sum(subtree.number_of_nodes() for subtree in subtrees) / len(subtrees) if subtrees else 0\n",
    "                ),\n",
    "                \"min_subtree_leaf_count_if_root\": (min(len(l) for l in subtree_leaves) if subtree_leaves else 0),\n",
    "                \"max_subtree_leaf_count_if_root\": (max(len(l) for l in subtree_leaves) if subtree_leaves else 0),\n",
    "                \"average_subtree_leaf_count_if_root\": (\n",
    "                    sum(len(l) for l in subtree_leaves) / len(subtree_leaves) if subtree_leaves else 0\n",
    "                ),\n",
    "                \"min_subtree_depth_if_root\": (min(subtree_depths) if subtree_depths else 0),\n",
    "                \"max_subtree_depth_if_root\": (max(subtree_depths) if subtree_depths else 0),\n",
    "                \"average_subtree_depth_if_root\": (sum(subtree_depths) / len(subtree_depths) if subtree_depths else 0),\n",
    "                \"depth_difference_if_root\": (max(subtree_depths) - min(subtree_depths)) if subtree_depths else 0,\n",
    "                \"number_of_subtrees_if_root\": len(subtrees),\n",
    "                # Centrality measures\n",
    "                \"eccentricity\": eccentricity[node],\n",
    "                \"closeness_centrality\": closeness_centrality[node],\n",
    "                #\"closeness_centrality_inverse\": 1 / closeness_centrality[node] if closeness_centrality[node] > 0 else 0,\n",
    "                \"degree_centrality\": degree_centrality[node],\n",
    "                \"harmonic_centrality\": harmonic_centrality[node],\n",
    "                \"betweenness_centrality\": betweenness_centrality[node],\n",
    "                \"pagerank\": pagerank[node],\n",
    "                \"katz_centrality\": katz_centrality[node],\n",
    "                \"current_flow_closeness\": current_flow_closeness[node],\n",
    "                \"current_flow_betweenness\": current_flow_betweenness[node],\n",
    "                \"load_centrality\": load_centrality[node],\n",
    "                \"percolation_centrality\": percolation_centrality[node],\n",
    "                \"second_order_centrality\": second_order_centrality[node],\n",
    "                \"laplacian_centrality\": laplacian_centrality[node],\n",
    "            }\n",
    "        )\n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60dcdd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation_unwound = pd.concat(validation.apply(unwind_tree, axis=1).tolist(), ignore_index=True)\n",
    "# validation_unwound[\"language\"] = validation_unwound[\"language\"].astype(\"category\")\n",
    "# validation_unwound.to_csv(\"../data/cache/validation_unwound.csv\", index=False)\n",
    "# validation_unwound.head()\n",
    "\n",
    "validation_unwound = pd.read_csv(\"../data/cache/validation_unwound.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "723b8b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_unwound = pd.concat(training.apply(unwind_tree, axis=1).tolist(), ignore_index=True)\n",
    "# training_unwound[\"language\"] = training_unwound[\"language\"].astype(\"category\")\n",
    "# training_unwound.to_csv(\"../data/cache/training_unwound.csv\", index=False)\n",
    "# training_unwound.head()\n",
    "\n",
    "training_unwound = pd.read_csv(\"../data/cache/training_unwound.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e308d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_unwound.to_csv(\"../data/cache/biel/training_unwound.csv\", index=False)\n",
    "# validation_unwound.to_csv(\"../data/cache/biel/validation_unwound.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c53d61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = training_unwound.drop(columns=[\"row_index\",\"node\",\"is_root\"])\n",
    "X_train = pd.get_dummies(X_train, columns=[\"language\"], drop_first=False) \n",
    "y_train = training_unwound[\"is_root\"]\n",
    "\n",
    "\n",
    "X_val = validation_unwound.drop(columns=[\"row_index\",\"node\",\"is_root\",])\n",
    "X_val = pd.get_dummies(X_val, columns=[\"language\"], drop_first=False)\n",
    "y_val = validation_unwound[\"is_root\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e7745cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node-based accuracy: 0.93\n",
      "Sentence-based accuracy: 0.26\n"
     ]
    }
   ],
   "source": [
    "lda = LinearDiscriminantAnalysis()\n",
    "lda.fit(X_train, y_train)\n",
    "\n",
    "predictions = lda.predict(X_val)\n",
    "print(f\"Node-based accuracy: {accuracy_score(y_val, predictions):.2f}\")\n",
    "\n",
    "\n",
    "sentence_predictions = defaultdict(dict)\n",
    "probs = lda.predict_proba(X_val)\n",
    "\n",
    "sentence_real_root = {}\n",
    "for (_, row), pred in zip(validation_unwound.iterrows(), probs):\n",
    "    sentence_predictions[row[\"row_index\"]][row[\"node\"]] = pred[1]\n",
    "    if row[\"is_root\"]:\n",
    "        sentence_real_root[row[\"row_index\"]] = row[\"node\"]\n",
    "\n",
    "if not set(sentence_predictions.keys()) == set(sentence_real_root.keys()):\n",
    "    raise ValueError(\"Mismatch between sentence predictions and real roots.\")\n",
    "\n",
    "\n",
    "def get_predicted_root(row: pd.Series) -> str:\n",
    "    \"\"\"\n",
    "    Get the predicted root node for a sentence.\n",
    "    \"\"\"\n",
    "    sentence_id = row.name\n",
    "    probs = sentence_predictions[sentence_id]\n",
    "    return max(probs.keys(), key=probs.get)\n",
    "\n",
    "\n",
    "validation_prediction = pd.DataFrame.from_dict(sentence_real_root, orient=\"index\", columns=[\"root\"])\n",
    "validation_prediction[\"predicted_root\"] = validation_prediction.apply(get_predicted_root, axis=1)\n",
    "print(\n",
    "    f\"Sentence-based accuracy: {accuracy_score(validation_prediction['root'], validation_prediction['predicted_root']):.2f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ae6d0836",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/discriminant_analysis.py:926: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node-based accuracy: 0.81\n",
      "Sentence-based accuracy: 0.19\n"
     ]
    }
   ],
   "source": [
    "qda =  QuadraticDiscriminantAnalysis()\n",
    "qda.fit(X_train, y_train)\n",
    "\n",
    "predictions = qda.predict(X_val)\n",
    "print(f\"Node-based accuracy: {accuracy_score(y_val, predictions):.2f}\")\n",
    "\n",
    "sentence_predictions = defaultdict(dict)\n",
    "probs = qda.predict_proba(X_val)\n",
    "\n",
    "sentence_real_root = {}\n",
    "for (_, row), pred in zip(validation_unwound.iterrows(), probs):\n",
    "    sentence_predictions[row[\"row_index\"]][row[\"node\"]] = pred[1]\n",
    "    if row[\"is_root\"]:\n",
    "        sentence_real_root[row[\"row_index\"]] = row[\"node\"]\n",
    "\n",
    "if not set(sentence_predictions.keys()) == set(sentence_real_root.keys()):\n",
    "    raise ValueError(\"Mismatch between sentence predictions and real roots.\")\n",
    "\n",
    "\n",
    "def get_predicted_root(row: pd.Series) -> str:\n",
    "    \"\"\"\n",
    "    Get the predicted root node for a sentence.\n",
    "    \"\"\"\n",
    "    sentence_id = row.name\n",
    "    probs = sentence_predictions[sentence_id]\n",
    "    return max(probs.keys(), key=probs.get)\n",
    "\n",
    "\n",
    "validation_prediction = pd.DataFrame.from_dict(sentence_real_root, orient=\"index\", columns=[\"root\"])\n",
    "validation_prediction[\"predicted_root\"] = validation_prediction.apply(get_predicted_root, axis=1)\n",
    "print(\n",
    "    f\"Sentence-based accuracy: {accuracy_score(validation_prediction['root'], validation_prediction['predicted_root']):.2f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ab54ecbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node-based accuracy: 0.78\n",
      "Sentence-based accuracy: 0.27\n"
     ]
    }
   ],
   "source": [
    "nb = GaussianNB()\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "predictions = nb.predict(X_val)\n",
    "print(f\"Node-based accuracy: {accuracy_score(y_val, predictions):.2f}\")\n",
    "\n",
    "\n",
    "sentence_predictions = defaultdict(dict)\n",
    "probs = nb.predict_proba(X_val)\n",
    "\n",
    "sentence_real_root = {}\n",
    "for (_, row), pred in zip(validation_unwound.iterrows(), probs):\n",
    "    sentence_predictions[row[\"row_index\"]][row[\"node\"]] = pred[1]\n",
    "    if row[\"is_root\"]:\n",
    "        sentence_real_root[row[\"row_index\"]] = row[\"node\"]\n",
    "\n",
    "if not set(sentence_predictions.keys()) == set(sentence_real_root.keys()):\n",
    "    raise ValueError(\"Mismatch between sentence predictions and real roots.\")\n",
    "\n",
    "\n",
    "def get_predicted_root(row: pd.Series) -> str:\n",
    "    \"\"\"\n",
    "    Get the predicted root node for a sentence.\n",
    "    \"\"\"\n",
    "    sentence_id = row.name\n",
    "    probs = sentence_predictions[sentence_id]\n",
    "    return max(probs.keys(), key=probs.get)\n",
    "\n",
    "\n",
    "validation_prediction = pd.DataFrame.from_dict(sentence_real_root, orient=\"index\", columns=[\"root\"])\n",
    "validation_prediction[\"predicted_root\"] = validation_prediction.apply(get_predicted_root, axis=1)\n",
    "print(\n",
    "    f\"Sentence-based accuracy: {accuracy_score(validation_prediction['root'], validation_prediction['predicted_root']):.2f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "99a09caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node-based accuracy: 0.95\n",
      "Sentence-based accuracy: 0.27\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(max_iter=5000)\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "predictions = lr.predict(X_val)\n",
    "print(f\"Node-based accuracy: {accuracy_score(y_val, predictions):.2f}\")\n",
    "\n",
    "\n",
    "sentence_predictions = defaultdict(dict)\n",
    "probs = lr.predict_proba(X_val)\n",
    "\n",
    "sentence_real_root = {}\n",
    "for (_, row), pred in zip(validation_unwound.iterrows(), probs):\n",
    "    sentence_predictions[row[\"row_index\"]][row[\"node\"]] = pred[1]\n",
    "    if row[\"is_root\"]:\n",
    "        sentence_real_root[row[\"row_index\"]] = row[\"node\"]\n",
    "\n",
    "if not set(sentence_predictions.keys()) == set(sentence_real_root.keys()):\n",
    "    raise ValueError(\"Mismatch between sentence predictions and real roots.\")\n",
    "\n",
    "\n",
    "def get_predicted_root(row: pd.Series) -> str:\n",
    "    \"\"\"\n",
    "    Get the predicted root node for a sentence.\n",
    "    \"\"\"\n",
    "    sentence_id = row.name\n",
    "    probs = sentence_predictions[sentence_id]\n",
    "    return max(probs.keys(), key=probs.get)\n",
    "\n",
    "\n",
    "validation_prediction = pd.DataFrame.from_dict(sentence_real_root, orient=\"index\", columns=[\"root\"])\n",
    "validation_prediction[\"predicted_root\"] = validation_prediction.apply(get_predicted_root, axis=1)\n",
    "print(\n",
    "    f\"Sentence-based accuracy: {accuracy_score(validation_prediction['root'], validation_prediction['predicted_root']):.2f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8f4f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC()\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "predictions = svm.predict(X_val)\n",
    "print(f\"Node-based accuracy: {accuracy_score(y_val, predictions):.2f}\")\n",
    "\n",
    "\n",
    "sentence_predictions = defaultdict(dict)\n",
    "probs = svm.predict_proba(X_val)\n",
    "sentence_real_root = {}\n",
    "\n",
    "for (_, row), pred in zip(validation_unwound.iterrows(), probs):\n",
    "    sentence_predictions[row[\"row_index\"]][row[\"node\"]] = pred[1]\n",
    "    if row[\"is_root\"]:\n",
    "        sentence_real_root[row[\"row_index\"]] = row[\"node\"]\n",
    "if not set(sentence_predictions.keys()) == set(sentence_real_root.keys()):\n",
    "    raise ValueError(\"Mismatch between sentence predictions and real roots.\")\n",
    "def get_predicted_root(row: pd.Series) -> str:\n",
    "    \"\"\"\n",
    "    Get the predicted root node for a sentence.\n",
    "    \"\"\"\n",
    "    sentence_id = row.name\n",
    "    probs = sentence_predictions[sentence_id]\n",
    "    return max(probs.keys(), key=probs.get)\n",
    "validation_prediction = pd.DataFrame.from_dict(sentence_real_root, orient=\"index\", columns=[\"root\"])\n",
    "validation_prediction[\"predicted_root\"] = validation_prediction.apply(get_predicted_root, axis=1)\n",
    "print(\n",
    "    f\"Sentence-based accuracy: {accuracy_score(validation_prediction['root'], validation_prediction['predicted_root']):.2f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87b43561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node-based accuracy: 0.95\n",
      "Sentence-based accuracy: 0.27\n"
     ]
    }
   ],
   "source": [
    "xgb = XGBClassifier(eval_metric='logloss')\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "predictions = xgb.predict(X_val)\n",
    "print(f\"Node-based accuracy: {accuracy_score(y_val, predictions):.2f}\")\n",
    "\n",
    "\n",
    "\n",
    "sentence_predictions = defaultdict(dict)\n",
    "probs = xgb.predict_proba(X_val)\n",
    "sentence_real_root = {}\n",
    "\n",
    "for (_, row), pred in zip(validation_unwound.iterrows(), probs):\n",
    "    sentence_predictions[row[\"row_index\"]][row[\"node\"]] = pred[1]\n",
    "    if row[\"is_root\"]:\n",
    "        sentence_real_root[row[\"row_index\"]] = row[\"node\"]\n",
    "if not set(sentence_predictions.keys()) == set(sentence_real_root.keys()):\n",
    "    raise ValueError(\"Mismatch between sentence predictions and real roots.\")\n",
    "\n",
    "def get_predicted_root(row: pd.Series) -> str:\n",
    "    \"\"\"\n",
    "    Get the predicted root node for a sentence.\n",
    "    \"\"\"\n",
    "    sentence_id = row.name\n",
    "    probs = sentence_predictions[sentence_id]\n",
    "    return max(probs.keys(), key=probs.get)\n",
    "validation_prediction = pd.DataFrame.from_dict(sentence_real_root, orient=\"index\", columns=[\"root\"])\n",
    "validation_prediction[\"predicted_root\"] = validation_prediction.apply(get_predicted_root, axis=1)\n",
    "print(\n",
    "    f\"Sentence-based accuracy: {accuracy_score(validation_prediction['root'], validation_prediction['predicted_root']):.2f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "abbb43d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 108 candidates, totalling 324 fits\n",
      "Best parameters found:  {'colsample_bytree': 0.8, 'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 200, 'subsample': 0.8}\n",
      "Node-based accuracy after tuning: 0.95\n",
      "Sentence-based accuracy after tuning: 0.26\n"
     ]
    }
   ],
   "source": [
    "#tune xgboost\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0]\n",
    "}\n",
    "grid_search = GridSearchCV(XGBClassifier(eval_metric='logloss'), param_grid, cv=3, scoring='f1', verbose=1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "best_xgb = grid_search.best_estimator_\n",
    "predictions = best_xgb.predict(X_val)\n",
    "print(f\"Node-based accuracy after tuning: {accuracy_score(y_val, predictions):.2f}\")\n",
    "sentence_predictions = defaultdict(dict)\n",
    "probs = best_xgb.predict_proba(X_val)\n",
    "sentence_real_root = {}\n",
    "\n",
    "for (_, row), pred in zip(validation_unwound.iterrows(), probs):\n",
    "    sentence_predictions[row[\"row_index\"]][row[\"node\"]] = pred[1]\n",
    "    if row[\"is_root\"]:\n",
    "        sentence_real_root[row[\"row_index\"]] = row[\"node\"]\n",
    "if not set(sentence_predictions.keys()) == set(sentence_real_root.keys()):\n",
    "    raise ValueError(\"Mismatch between sentence predictions and real roots.\")\n",
    "\n",
    "def get_predicted_root(row: pd.Series) -> str:\n",
    "    \"\"\"\n",
    "    Get the predicted root node for a sentence.\n",
    "    \"\"\"\n",
    "    sentence_id = row.name\n",
    "    probs = sentence_predictions[sentence_id]\n",
    "    return max(probs.keys(), key=probs.get)\n",
    "validation_prediction = pd.DataFrame.from_dict(sentence_real_root, orient=\"index\", columns=[\"root\"])\n",
    "validation_prediction[\"predicted_root\"] = validation_prediction.apply(get_predicted_root, axis=1)\n",
    "print(\n",
    "    f\"Sentence-based accuracy after tuning: {accuracy_score(validation_prediction['root'], validation_prediction['predicted_root']):.2f}\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
